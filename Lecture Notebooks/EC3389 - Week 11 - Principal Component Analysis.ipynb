{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_style(\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some helper functions. Don't worry about them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_T1T2T3(v):\n",
    "    fig, ax = plt.subplots(1, 3, figsize = (15,5))\n",
    "    plot_arrows(T1, v, ax[0])\n",
    "    ax[0].set_title(\"$T_{1}v$\", fontsize = 20)\n",
    "    plot_arrows(T2, v, ax[1])\n",
    "    ax[1].set_title(\"$T_{2}v$\", fontsize = 20)\n",
    "    plot_arrows(T3, v, ax[2])\n",
    "    ax[2].set_title(\"$T_{3}v$\", fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_arrows(T, v, ax):\n",
    "    Tv = T.dot(v)\n",
    "    ax.arrow(0, 0, v[0,0], v[1,0], head_width=.5, head_length=.5, width = .1, fc='darkorange', ec='darkorange', alpha = .3)\n",
    "    ax.arrow(0, 0, Tv[0,0], Tv[1,0], head_width=.5, head_length=.5, width = .1, fc='darkorange', ec='darkorange')\n",
    "    m = np.max([np.abs(np.max(v)), np.abs(np.max(Tv))])\n",
    "    ax.set_xlim([-2*m, 2*m])\n",
    "    ax.set_ylim([-2*m, 2*m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_multiple_applications(T, v, n_iter = 10):\n",
    "    fig, ax = plt.subplots(1, figsize = (10, 10))\n",
    "    colors = plt.cm.inferno_r(np.linspace(0.2, 1, n_iter))\n",
    "    ax.scatter(v[0,0], v[1,0], s = 300, marker = \"*\", color = colors[0,:])\n",
    "    Tv = T.dot(v)\n",
    "    for i in range(n_iter):\n",
    "        ax.scatter(Tv[0,0], Tv[1,0], color = colors[i,:], s= 100)\n",
    "        Tv = T.dot(Tv)\n",
    "        \n",
    "    ax.scatter(Tv[0,0], Tv[1,0], color = colors[i,:], s = 500, marker = \"*\")\n",
    "    ax.set_title(\"Multiple applications of $T$\", fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_data_transformation(T):\n",
    "    x = np.random.multivariate_normal(mean = [0,0], cov = [[1,0],[0,1]], size = 500)\n",
    "    Tx = (T.dot(x.T)).T\n",
    "    \n",
    "    fig, ax  = plt.subplots(1, 2, figsize = (20, 8), sharex = True, sharey = True)\n",
    "    ax[0].scatter(x[:,0], x[:,1], c = \"darkgreen\", s = 200, alpha = .3)\n",
    "    ax[0].scatter(x[0,0], x[0,1], c = \"darkblue\", s = 200, alpha = .8)\n",
    "    ax[0].set_title(\"$x$\", fontsize = 40)\n",
    "    ax[1].scatter(Tx[:,0], Tx[:,1], c = \"darkgreen\", s = 200, alpha = .3)\n",
    "    ax[1].scatter(Tx[0,0], Tx[0,1], c = \"darkblue\", s = 200, alpha = .8)\n",
    "    ax[1].set_title(\"$Tx$\", fontsize = 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Week 11 - Principal Component Analysis </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Warm-up: What are matrices? </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Matrices as *data*\n",
    "\n",
    "+ What does a matrix *do*?\n",
    "\n",
    "+ <font size = 1> [Bonus question] What does a <a href=\"https://www.youtube.com/watch?v=F_0yfvm0UoU#t=1m05s\">number</a> do?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>1) Linear transformations</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "T(\\alpha v + \\beta w) = \\alpha T(v) + \\beta T(w) \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<b>Examples of linear transformations</b>\n",
    "\n",
    "+ Stretching? \n",
    "\n",
    "+ Rotation?\n",
    "\n",
    "+ Squaring?\n",
    "\n",
    "+ Exponentiating?\n",
    "\n",
    "+ Translation?\n",
    "\n",
    "+ Derivatives? Integration?\n",
    "\n",
    "+ Matrix multiplication?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do these matrices *do* to a vector?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "T_{1} = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0& 1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\qquad\n",
    "T_{2} = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & -1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\qquad\n",
    "T_{3} = \n",
    "\\frac{\\sqrt{2}}{2}\n",
    "\\begin{bmatrix}\n",
    "1 & -1 \\\\\n",
    "1 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T1 = np.array([[1, 0],[0, 1]])\n",
    "T2 = np.array([[1, 0],[0, -1]])\n",
    "T3 = np.sqrt(2)/2  * np.array([[1,-1],[1,1]])\n",
    "\n",
    "v1 = np.array([[1],[0]])\n",
    "v2 = np.array([[0],[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see that by applying to \"simple\" vectors like $(1,0)$ and $(0,1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_T1T2T3(v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_T1T2T3(v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Try out some more vectors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come up with other matrices yourself and see what they do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "T = np.array([[1,1],[0,1]]) # Change this...\n",
    "v = np.array([[1],[1]]) # ...and this\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize = (5,5))\n",
    "plot_arrows(T, v, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The path traced out by $v$, $Tv$, $TTv$,$TTTv$, $\\cdots$ is also interesting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T = 0.9*np.array([[1, -1], [1,1]])\n",
    "v = np.array([[1],[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_multiple_applications(T, v, n_iter = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linearly transforming the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose $T$ is any linear transformation and $x \\sim \\mathcal{N}(0, I)$. Then what does $Tx$ look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T = np.array([[1,0],[0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_data_transformation(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Discussion</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ [Important!] What can we say about the covariance of $Tx$? \n",
    "\n",
    "+ Did we need the normality assumption?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Eigenvectors, eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the different between the blue / pink vectors, versus the red ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/ad/Eigenvectors-extended.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the difference between the red and blue vectors below?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/3/3c/Mona_Lisa_eigenvector_grid.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To *project* means *to forget some coordinate*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize = (5,5))\n",
    "ax.arrow(0, 0, 2, 0, head_width=.2, head_length=.2, width = .1, fc='darkorange', ec='darkorange', alpha = 1)\n",
    "ax.arrow(0, 0, 0, 3, head_width=.2, head_length=.2, width = .1, fc='darkorange', ec='darkorange', alpha = 1)\n",
    "ax.arrow(0, 0, 2, 3, head_width=.2, head_length=.2, width = .1, fc='navy', ec='navy', alpha = 1)\n",
    "ax.set_xlim(-0.1, 3)\n",
    "ax.set_ylim(-0.1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, note that we can decompose $v$ as:\n",
    "$$\n",
    "v = \n",
    "\\begin{bmatrix}\n",
    "2 \\\\ 3\n",
    "\\end{bmatrix}\n",
    "=\n",
    "2\n",
    "\\begin{bmatrix}\n",
    "1 \\\\ 0\n",
    "\\end{bmatrix}\n",
    "+\n",
    "3\n",
    "\\begin{bmatrix}\n",
    "0 \\\\ 1\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Projection onto the horizontal axis: \n",
    "$\n",
    "\\qquad\n",
    "v_{x} = \n",
    "\\begin{bmatrix}\n",
    "2 \\\\ 0\n",
    "\\end{bmatrix}\n",
    "=\n",
    "2\n",
    "\\begin{bmatrix}\n",
    "1 \\\\ 0\n",
    "\\end{bmatrix}\n",
    "+\n",
    "0\n",
    "\\begin{bmatrix}\n",
    "0 \\\\ 1\n",
    "\\end{bmatrix}\n",
    "\\qquad\n",
    "$\n",
    "(kills the $y$ coordinate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Projection onto the vertical axis: \n",
    "$\n",
    "\\qquad\n",
    "v_{y} = \n",
    "\\begin{bmatrix}\n",
    "0 \\\\ 3\n",
    "\\end{bmatrix}\n",
    "=\n",
    "0\n",
    "\\begin{bmatrix}\n",
    "1 \\\\ 0\n",
    "\\end{bmatrix}\n",
    "+\n",
    "3\n",
    "\\begin{bmatrix}\n",
    "0 \\\\ 1\n",
    "\\end{bmatrix}\n",
    "\\qquad\n",
    "$\n",
    "(kills the $x$ coordinate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 1>By the way, regression is a kind of projection, where we \"forget\" the $\\epsilon$ coordinate. Trippy.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So what about $\\Sigma$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a moment to recap some of what we've learned:\n",
    "\n",
    "+ Linear transformations stretch or rotate the data\n",
    "\n",
    "+ Linear transformations of regressors are related to their covariance\n",
    "\n",
    "+ Every linear transformation has an invariant direction that is encoded by the *eigenvector*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So a more or less natural question is: what are the eigenvectors of the covariance matrix? How do they relate to the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take any valid covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sigma = np.array([[2,1],[1,2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw some data using this covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.random.multivariate_normal(mean = [0,0], cov = sigma, size = (500,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the eigenvectors, eigenvalues of the covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eigenvalues, eigenvectors = np.linalg.eig(sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize = (8, 8))\n",
    "ax.scatter(X[:,0], X[:,1], alpha = .5)\n",
    "\n",
    "ax.arrow(0, 0, eigenvalues[0]*eigenvectors[0,0], eigenvalues[0]*eigenvectors[1,0],  \n",
    "         head_width=1, head_length=1, width = .2, fc='k', ec='k')\n",
    "ax.arrow(0, 0, eigenvalues[1]*eigenvectors[0,1], eigenvalues[1]*eigenvectors[1,1], \n",
    "         head_width=1, head_length=1, width = .2, fc='k', ec='k')\n",
    "ax.set_title(\"Eigenvectors of the covariance matrix\", fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So how about PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>PCA is projection onto smart coordinates</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.random.multivariate_normal(mean = [0,0], cov = [[5,4],[4,5]], size = (500,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I asked you to summarize $X_{1}$ and $X_{2}$ by *one* number, how would you do it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize = (8,5))\n",
    "ax.scatter(X[:,0], X[:,1])\n",
    "ax.set_xlabel(\"X1\", fontsize = 20)\n",
    "ax.set_ylabel(\"X2\", fontsize = 20)\n",
    "sns.rugplot(X[:,1], ax= ax)\n",
    "ax.set_title(\"Not a smart projection\", fontsize = 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Why is this not a \"smart\" projection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: rotate first, then project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Rotate how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[I'll do the rest on the board]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA in sci-kit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "import sklearn.decomposition as skd\n",
    "mnist = fetch_mldata('MNIST original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "threes = mnist[\"data\"][mnist[\"target\"] == 3,: ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Scikit-learn</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = skd.PCA(n_components= 9, whiten = True)\n",
    "pca.fit(threes)  # No \"predict\" since unsupervised learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3,3, figsize = (15,15))\n",
    "fig.suptitle(\"$Eigenthrees$\", fontsize = 25)\n",
    "for k, comp in enumerate(pca.components_):\n",
    "    i,j = np.unravel_index(k, ax.shape)\n",
    "    ax[i,j].imshow(comp.reshape(28, 28), cmap = plt.cm.gray_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ What are the first two components responsible for? How would an observation heavy on the first component look like? How about the second?\n",
    "\n",
    "+ Why did I (jokingly) call this \"eigenthrees\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Fraction of variance explained</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we really don't need the smaller components!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = skd.PCA(n_components= 100)\n",
    "pca.fit(threes)  # No \"predict\" since unsupervised learning!\n",
    "plt.plot(range(pca.n_components_), pca.explained_variance_)\n",
    "plt.xlabel('Number of components', fontsize = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More cool stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Can any linear transformation be represented by a matrix?\n",
    "\n",
    "+ Differentiation is linear:\n",
    "\n",
    "$$\\frac{d}{dx}(\\alpha f(x) + \\beta g(x)) = \\alpha \\frac{d}{dx}f + \\beta \\frac{d}{dx}g(x)$$\n",
    "\n",
    "so can differentiation be \"represented by a matrix\"? If it can, then what are its \"eigenfunctions\"?\n",
    "\n",
    "+ It looks like we can represent many things with \"linear combinations\". What field studies this sort of thing?\n",
    "\n",
    "+ [Take home] Why did I ask, way in the beginning, \"what is a number\"?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
