{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> EC3389 - Week 6 - Cross-validation </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.cross_validation as skv\n",
    "import sklearn.linear_model as skl\n",
    "import sklearn.neighbors as skn\n",
    "import sklearn.metrics as skm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise</b>  Create a function <font face=\"Courier\">generate_data</font> that takes an integer <font face=\"Courier\">n_points</font> a float $a$, and a positive float $\\sigma$, and returns a tuple of three numpy arrays:\n",
    "\n",
    "+ An array $X$ of <font face= \"Courier\">n_points</font> linearly spaced data on $[0,\\pi]$\n",
    "\n",
    "+ An array $y\\_true_{i} = cos(a x_{i})$\n",
    "\n",
    "+ An array $y_{i} = cos(a x_{i}) + \\epsilon_{i}$ where $\\epsilon_{t} \\sim \\mathcal{N}(0,\\sigma^2)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data(n_points, a = 1, sigma = .1):\n",
    "    \"\"\"\n",
    "    Generate the function here\n",
    "    \"\"\"\n",
    "    return X, y_true, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then generate and plot $(X,y_{1})$ as a solid line, and $(X,y_{2})$ as a scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X, y_true, y = generate_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation using K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise</b> Estimate this model using <font face =\"Courier\">sklearn.neighbors.KNeighborsRegressor</font> with five neighbors. \n",
    "\n",
    "Use the pair <font face =\"Courier\">(X, y)</font> as data. \n",
    "\n",
    "Then print your predicted model against <font face =\"Courier\">y_true</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn = skn.KNeighborsRegressor()\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.plot(X, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Discussion</b> \n",
    "\n",
    "+ What happens to the model as you change $K$, the number of neighbors? \n",
    "\n",
    "+ Specifically, what happens at the extremes $K \\rightarrow 1$ and $K \\rightarrow \\infty$? \n",
    "\n",
    "+ How can we choose the optimal $K$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 1>Note: The \"K\" in \"K-fold\" has nothing to do with the \"K\" in \"K-nearest neighbors\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise</b> Check out the function <font face=\"Courier\">sklearn.cross_validation.KFold</font>. What does each of its arguments do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kf = skv.KFold(n = 8, n_folds = 4, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for train_index, test_index in kf:\n",
    "    print(train_index, test_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Discussion</b> \n",
    "\n",
    "+ Why would this be useful? Compare with just holding out a test set as we did last time.\n",
    "\n",
    "+ Specifically, if we were to divide the data we just created, should we set <font face=\"Courier\">shuffle</font> to  <font face=\"Courier\">True</font> or <font face=\"Courier\">False</font>?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise (1) </b> Use <font face=\"Courier\">KFold</font> to divide your data into 3 folds. \n",
    "\n",
    "Train using <font face=\"Courier\">sklearn.linear_model.KNeighborRegressor</font> on each training set. \n",
    "\n",
    "Each time you train the model, <font face=\"Courier\">sklearn.metrics.mean_squared_error</font> to compute and print the $MSE$ on train and test sets.\n",
    "\n",
    "\n",
    "<b>Exercise (2) </b> Redo the exercise, but now increase the number of folds. What happens to the $R^2$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_obs = 1000\n",
    "\n",
    "X, y_true, y = generate_data(n_obs, a = 1, sigma = .1)\n",
    "\n",
    "kf = skv.KFold(n = n_obs, n_folds = 50, shuffle = True)\n",
    "\n",
    "knn = skn.KNeighborsRegressor(n_neighbors = 50) # Or any other number of neighbors\n",
    "\n",
    "for train_index, test_index in kf:\n",
    "    \n",
    "    # Get your train and test data for this iteration\n",
    "    # X_train, y_train = \n",
    "    # X_test, y_test = \n",
    "    \n",
    "    # Fit the model on training set\n",
    "\n",
    "    # Predict on test and training set\n",
    "    \n",
    "    # Get the MSE\n",
    "    # mse_test = skm.mean_squared_error(y_true = , y_pred = )\n",
    "    # mse_train = skm.mean_squared_error(y_true = , y_pred = )\n",
    "    \n",
    "    # Print it out\n",
    "    #print(\"Train MSE\", mse_train, \"; Test MSE\", mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Discussion</b> Many statisticians and machine learning practitioners advocate for a number of folds of the order of $K = 5$ or $K = 10$. However, the standard in econometrics is *leave-one-out* practice of $K = n$. Name some arguments for and against either of these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 1>Sit back and relax: I've got the rest of the code for today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it's useful to plot the influence of a single tuning parameter on the training score and validation scores to find out whether the estimator is overfitting or underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y_true, y = generate_data(1000, a = 3, sigma = 1)\n",
    "n_neighbors_list = [500, 400, 300, 200, 100, 50, 25, 5]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(4, 2, figsize = (15,10))\n",
    "fig.subplots_adjust(hspace = .5)\n",
    "\n",
    "colors = plt.cm.Dark2(np.linspace(0,1, len(n_neighbors_list)))\n",
    "\n",
    "for k, n_neighbors in enumerate(n_neighbors_list):\n",
    "    \n",
    "    knn = skn.KNeighborsRegressor(n_neighbors = n_neighbors)\n",
    "    knn.fit(X, y)\n",
    "    yhat = knn.predict(X)\n",
    "    \n",
    "    i,j = np.unravel_index(k, ax.shape)\n",
    "    ax[i,j].plot(X, y_true, linewidth = 2, color = \"black\")\n",
    "    ax[i,j].plot(X, yhat, linewidth = 2, color = colors[k])\n",
    "    ax[i,j].set_title(label = str(n_neighbors), fontsize = 16)\n",
    "    sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Discussion</b> \n",
    "\n",
    "+ For which values is this <i>under</i>fitting? How about <i>over</i>fitting? \n",
    "\n",
    "+ What is your guess for the optimal value of neighbors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y_true, y = generate_data(1000, a = 2, sigma = .1)\n",
    "\n",
    "n_neighbors_list = [500, 400, 300, 200, 100, 50, 25, 5]\n",
    "\n",
    "train_scores, test_scores = skc.validation_curve(estimator = skn.KNeighborsRegressor(),\n",
    "                                                 X = X, y = y,\n",
    "                                                 param_name = \"n_neighbors\", \n",
    "                                                 param_range = n_neighbors_list,\n",
    "                                                 scoring = \"mean_squared_error\", \n",
    "                                                 cv = 3)\n",
    "\n",
    "# Note the totally counter-intuitive sign flipping here!\n",
    "mse_train = -train_scores\n",
    "mse_test = -test_scores\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "# Plotting the mean of the CV-estimated MSE on test and training set\n",
    "p = ax.plot(mse_test.mean(axis = 1), color = \"orange\", label = \"test\")\n",
    "p = ax.plot(mse_train.mean(axis = 1), color = \"blue\", label = \"train\")\n",
    "\n",
    "\n",
    "p = ax.set_xticklabels(n_neighbors_list,  size='large')\n",
    "p = ax.set_xlabel(\"Flexibility (Number of neighbors)\", fontsize = 14)\n",
    "p = ax.set_title(\"MSE\", fontsize = 16)\n",
    "p = ax.legend(fontsize = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Discussion</b> \n",
    "\n",
    "+ What is contributing for the increasing in test error as the model becomes more flexible?\n",
    "\n",
    "+ What happens if we increase the number of observations?\n",
    "\n",
    "+ What happens if we change $a$ in <font face=\"Courier\">generate_data</font>?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"navy\"><b>Advanced</b></font> Can we improve our estimates of the MSE? Yes, using <i>bootstrapping</i>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
